{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get access to the xml file of the sentences for the task.\n",
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse(\"lexsub_trial.xml\")\n",
    "root = tree.getroot()\n",
    "\n",
    "#Getting a list of the sentences with nouns as heads.\n",
    "noun_stuff = []\n",
    "for child in root:\n",
    "    word = child.attrib['item']\n",
    "    if word.endswith('.n'):\n",
    "        noun_stuff.append(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import semcor as sc\n",
    "from nltk.tokenize import word_tokenize as wt\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a list of the words of interest.\n",
    "heads_list = []\n",
    "for i in range(len(noun_stuff)):\n",
    "    for j in range(len(noun_stuff[i])):\n",
    "        word = wnl.lemmatize(noun_stuff[i][j][0][0].text.lower())\n",
    "        if word not in heads_list:\n",
    "            heads_list.append(word)\n",
    "\n",
    "#Create a list of the synonyms of interest, from the list of head words.\n",
    "synsets = []\n",
    "for h in heads_list:\n",
    "    syns = [s.name() for s in wn.synsets(h, pos=wn.NOUN)]\n",
    "    synsets += syns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A function taking a list of synsets of interest, and returning a dictionary of word senses and corresponding \n",
    "#context from semcor.\n",
    "\n",
    "def get_sense_dict(synsets):\n",
    "    sc_ids = sc.fileids() #get all the semcor fileids\n",
    "    lookup = {} #initialise a dictionary\n",
    "    \n",
    "    #Search through all the sentences in all the files for any words sense-tagged with a synset of interest.\n",
    "    for id in sc_ids: #for each id\n",
    "        sents = sc.xml(id).findall('context/p/s') #get all of the sentences in the file, in xml form\n",
    "        sents_text = sc.sents(id) #get all of the sentences in the file, as a list of strings\n",
    "        \n",
    "        for i in range(len(sents)): #for each sentence\n",
    "            for wordform in sents[i].getchildren(): #for each word in the sentence\n",
    "                if wordform.get('lexsn'): #if the word has been assigned a lex_sense\n",
    "                    syn_key = wordform.get('lemma') + '%' + wordform.get('lexsn') #generate the synset key for the word\n",
    "                    \n",
    "                    try: #try to use that key to find the corresponding synset\n",
    "                        s = wn.lemma_from_key(syn_key).synset().name()\n",
    "                    except Exception: #if that doesn't work, construct a synset ID from other info\n",
    "                        try:\n",
    "                            sense = '%s.%s.%02d' % (wordform.get('lemma'), wordform.get('pos'), int(wordform.get('wnsn')))\n",
    "                        except ValueError: #if the system doesn't like the int form of wnsn (happens sometimes)\n",
    "                            sense = wordform.get('lemma')+'.'+wordform.get('pos')+'.'+wordform.get('wnsn')\n",
    "                            \n",
    "                    if s in synsets: #if the synset is one of the ones we're interested in...\n",
    "                        if s in lookup: #if already in the dictionary, add the rest of the sentence to the value entry.\n",
    "                            lookup[s] += [w for w in sents_text[i] if w not in heads_list]\n",
    "                        else: #otherwise create new key,value pairing, using the synset and rest of the sentence as value.\n",
    "                            lookup[s] = [w for w in sents_text[i] if w not in heads_list]\n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate the dictionary of synsets and contexts from semcor.\n",
    "sense_dict = get_sense_dict(synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#A function taking a sentence with word to be replaced as input, and returning a replacement word.\n",
    "#Calls a number of sub-functions to do its work (see below).\n",
    "\n",
    "#PARAMETERS:\n",
    "\n",
    "#syn_approach - 1=take most frequent synset as sense; 2=use knowledge to try and identify sense.\n",
    "\n",
    "#depth - 1=synset glosses to contain description and examples for synset alone; \n",
    "#...2=glosses include hyponym description and examples; 3=hypernym description and examples added;\n",
    "#...4=words from SemCor sentences containing a word with given synset added to glosses;\n",
    "#...5=only synset description and examples + SemCor words in glosses. To be passed to get_synset,\n",
    "#and then on to get_thesaurus_stuff functions.\n",
    "\n",
    "#lemm_approach - 1=take first alternative lemma WordNet gives for synset;\n",
    "#...2=take most frequently occuring lemma of the alternatives;\n",
    "#...3=take lemma with highest distributional similarity to target word (using Word2Vec).\n",
    "#To be passed to the get_lemma function.\n",
    "\n",
    "def get_replacement(instance, syn_approach=1, depth=1, lemm_approach=1):\n",
    "    \n",
    "    #Treat a lemmatised version of the head as the word to be replaced, and get the word's synsets.\n",
    "    word = wnl.lemmatize(instance[0][0].text.lower())\n",
    "    syns = wn.synsets(word, wn.NOUN)\n",
    "    \n",
    "    #If chosen approach is just to take most common synset, return the first in the list.\n",
    "    if syn_approach==1:\n",
    "        synset = syns[0]\n",
    "        \n",
    "    #Otherwise, get the context the word appears in and use this to identify likely sense, and best synset.\n",
    "    else:    \n",
    "        #Treat the text either side of the head word as the context.\n",
    "        if instance[0].text: #if the head word isn't the first of the sentence, context is what comes before and after it\n",
    "            context = instance[0].text, instance[0][0].tail\n",
    "            context = wt(' '.join(context))\n",
    "        else: #if head is first of sentence, context is whatever comes after it\n",
    "            context = wt(instance[0][0].tail)\n",
    "        context = [w.lower() for w in context if w.isalpha() and w not in stopwords]\n",
    "    \n",
    "        synset = get_synset(syns, context, depth) #pass synsets and context to get_synset function\n",
    "        \n",
    "    #Given the synset returned by get_synset, find an alternative lemma\n",
    "    lemma = get_lemma(word, synset, lemm_approach)\n",
    "    \n",
    "    #If no lemma is returned, likely because the only lemma for given synset was original word, use hypernyms\n",
    "    #of the sense to find a replacement word.\n",
    "    if not lemma:\n",
    "        hyper_syns = synset.hypernyms()\n",
    "\n",
    "        if len(hyper_syns) > 1: #If there is more than one hypernym:\n",
    "            synset = get_synset(hyper_syns, context, depth) #Try to identify which hypernym is best.\n",
    "                \n",
    "        else: #Otherwise just use the hypernym as the new synset.\n",
    "            synset = hyper_syns.pop()\n",
    "                \n",
    "        lemma = get_lemma(word, synset, lemm_approach) #Find the best lemma from the hypernym synset.   \n",
    "    \n",
    "    return lemma #Return whichever lemma was found as the best replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A function to find the best synset given context. Follows simplified Lesk algorithm, comparing the context the\n",
    "#word appears in with the glosses and examples from WordNet and sentences from SemCor corresponding to each sense.\n",
    "\n",
    "#PARAMETERS:\n",
    "\n",
    "#synsets - the set of WordNet synsets for the target word.\n",
    "#context - the context (words in sentence) the target word appears in.\n",
    "#depth - see description above get_replacement function; to be passed to get_thesaurus_stuff.\n",
    "\n",
    "def get_synset(synsets, context, depth):\n",
    "    max_count = 0 #a value to monitor the best crossover score\n",
    "    best_set = None #to update with best synset, based on crossover score\n",
    "\n",
    "    for k in range(len(synsets)): #for each synset\n",
    "        syn_gloss = get_thesaurus_stuff(synsets[k], depth) #get the gloss\n",
    "        crossover = [w for w in context if w in syn_gloss] #pick out words appearing in gloss and target context\n",
    "        count = len(crossover) #crossover count is length of list of crossover words\n",
    "\n",
    "        if count > max_count: #if this is the largest crossover so far\n",
    "            max_count = count #update the best crossover score accordingly...\n",
    "            best_set = synsets[k] #and do the same for the best synset.\n",
    "    \n",
    "    if not best_set: #if no best set is identified (if there is no crossover for any of them)\n",
    "        best_set = synsets[0] #pick the first WordNet synset as the best\n",
    "\n",
    "    return best_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A function to get the thesaurus gloss for a given synset. This can be expanded to include that\n",
    "#of its hypo- and hypernyms, and words from SemCor sentences containing the sense.\n",
    "\n",
    "#PARAMETERS:\n",
    "\n",
    "#synset - the synset in question, from get_synset.\n",
    "#depth - see description above get_replacement function; from get_replacement via get_synset.\n",
    "\n",
    "def get_thesaurus_stuff(synset, depth):\n",
    "    main_def = wt(synset.definition()) #get the definition for the WordNet synset\n",
    "    main_examples = wt(' '.join(synset.examples())) #get the examples for the WordNet synset\n",
    "    def_eg = main_def + main_examples #combine the words in the definition and examples\n",
    "    gloss = [w.lower() for w in def_eg if w.isalpha() and w not in stopwords] #convert to lower case, prune stopwords\n",
    "    \n",
    "    if depth > 1 and depth < 5: #if info from hyponyms is to be added\n",
    "        \n",
    "        hyponyms = synset.hyponyms() #get the hyponyms of the synset\n",
    "        for l in range(len(hyponyms)): #for each hyponym\n",
    "            definition = wt(hyponyms[l].definition()) #get the definition for the hyponym\n",
    "            joined_examples = wt(' '.join(hyponyms[l].examples())) #get the examples for the hyponym\n",
    "            stuff = definition + joined_examples #combine the words in the definition and examples\n",
    "            bag = [w.lower() for w in stuff if w.isalpha() and w not in stopwords]\n",
    "            gloss += bag #add to gloss\n",
    "            \n",
    "    if depth > 2 and depth < 5: #do the same again for hypernyms, if selected.\n",
    "        \n",
    "        hypernyms = synset.synonyms()\n",
    "        for l in range(len(hypernyms)):\n",
    "            definition = wt(hypernyms[l].definition())\n",
    "            joined_examples = wt(' '.join(hypernyms[l].examples()))\n",
    "            stuff = definition + joined_examples\n",
    "            bag = [w.lower() for w in stuff if w.isalpha() and w not in stopwords]\n",
    "            gloss += bag\n",
    "    \n",
    "    if depth > 3: #if words from SemCor sentences are to be added to the gloss\n",
    "        if synset.name() in sense_dict: #lookup the synset in the dictionary from get_sense_dict\n",
    "            semcor_words = [w.lower() for w in sense_dict[synset.name()] if w not in stopwords] #prune stopwords\n",
    "            gloss += semcor_words #add to gloss\n",
    "            \n",
    "    return gloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import vectors and libraries for distributional similarity measure in get_lemma function.\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "filename=\"GoogleNews-vectors-negative300.bin\"\n",
    "mymodel = gensim.models.KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Function to find the best lemma from the given synset to use as the word's replacement.\n",
    "\n",
    "#PARAMETERS:\n",
    "\n",
    "#word - the target word, from get_replacement.\n",
    "#synset - the synset selected as matching the target word's sense, from get_replacement.\n",
    "#approach - see lemm_approach description above get_replacement.\n",
    "\n",
    "def get_lemma(word, synset, approach):\n",
    "\n",
    "    poss_lemmas1 = [l for l in synset.lemmas() if str(l.name())!=word] #list of possible alternative lemmas, in lemma form\n",
    "    lemmas = [str(lemma.name()) for lemma in synset.lemmas()] #list of wordforms for lemmas associated with synset\n",
    "    poss_lemmas2 = [l.lower() for l in lemmas if l.lower()!=word] #list of possible alternative words\n",
    "    \n",
    "    if len(poss_lemmas2)== 0: #if there are no alternative lemmas\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "         \n",
    "        if approach==1: #just return first alternative word\n",
    "            return poss_lemmas2[0]\n",
    "\n",
    "        if approach==2: #find the lemma with the highest WordNet count\n",
    "            max_count = 0\n",
    "            best = None\n",
    "            for lem in poss_lemmas1: #for each possible lemma\n",
    "                count = lem.count() #get its count\n",
    "                if count > max_count:\n",
    "                    best = str(lem.name())\n",
    "                    max_count = count\n",
    "                    \n",
    "        else: #find the lemma deemed most similar to the target word\n",
    "            max_sim = 0\n",
    "            best = None\n",
    "            for lem in poss_lemmas2: #for each possible lemma\n",
    "                try: #get a similarity score between it and the target word\n",
    "                    sim = mymodel.similarity(word, lem)\n",
    "                except Exception: #if, for instance, either word doesn't appear in the vocabulary\n",
    "                    sim = 0 #just make the similarity score 0\n",
    "                if sim > max_sim:\n",
    "                    max_sim = sim\n",
    "                    best = lem\n",
    "                    \n",
    "        if best is None: #if no best lemma is identified\n",
    "            best = poss_lemmas2[0] #just pick the first one.\n",
    "                    \n",
    "        return best"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
